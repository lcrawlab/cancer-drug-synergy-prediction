{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the data #\n",
    "\n",
    "Pre-requisites to using this file\n",
    "- Run the string_preprocessing.py main function and store the STRING database with known interactions as data_processed/STRING_full_filtered.tsv\n",
    "- Manually retrieve STRING IDs to UniProtIDs and UniProtIDs to GeneIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from preprocessing_files.preprocess_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the DNA, RNA, and Protein data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_dna_data()\n",
    "preprocess_rna_data()\n",
    "preprocess_protein_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the string_preprocessing.py main function before this!\n",
    "\n",
    "Get all of the STRING IDs from the full filtered STRING and save to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_STRING = pd.read_csv('data_processed/STRING_full_filtered.tsv', sep='\\t')\n",
    "# Get all STRING IDs in both columns protein1 and protein2\n",
    "known_STRING_ids = set(known_STRING['protein1']).union(set(known_STRING['protein2']))\n",
    "# Save all STRING IDs in a file\n",
    "with open('data_processed/known_STRING_ids.txt', 'w') as f:\n",
    "    for id in known_STRING_ids:\n",
    "        f.write(id + '\\n')\n",
    "\n",
    "print('Number of known STRING IDs: {}'.format(len(known_STRING_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually get the STRING id to UniProt ID conversion from https://www.uniprot.org/id-mapping/ ###\n",
    "\n",
    "Manually retrieved STRING IDs to UniProtKB -- 18,305 IDs were mapped to 18,305 results, but 422 ID were not mapped\n",
    "- Mapped STRING IDs saved at \"data_processed/stringids_to_uniprotkb.tsv\"\n",
    "- Unmapped STRING IDs saved at \"data_processed/STRING_unmapped_stringids.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_uniprot = pd.read_csv('data_processed/stringids_to_uniprotkb.tsv', sep='\\t')\n",
    "# Get all UniProtKB IDs in \"Entry\" column\n",
    "mapped_uniprot_ids = set(mapped_uniprot['Entry'])\n",
    "# Save all UniProtKB IDs in a file\n",
    "with open('data_processed/STRING_mapped_uniprot_ids.txt', 'w') as f:\n",
    "    for id in mapped_uniprot_ids:\n",
    "        f.write(id + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually get the UniProt ID conversion to GeneID from https://www.uniprot.org/id-mapping/ ###\n",
    "\n",
    "UniProtKB AC/ID to GeneID -- 17,816 IDs were mapped to 18,018 results, but 489 ID were not mapped\n",
    "- Mapped UniProtIDs saved at \"data_processed/uniprot_ids_to_gene_id.tsv\"\n",
    "- Unmapped UniProtIDs saved at \"data_processed/STRING_unmapped_uniprotids.txt\"\n",
    "\n",
    "The mapped gene IDs have significant intersection with the DNA/RNA/Protein intersection: found 2638 out of 2667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the entrez IDs for the IDs that couldn't be mapped. Go from STRING ID -> Protein Symbol (assume same as gene name) -> Entrez ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all gene names to entrez IDs possible\n",
    "gene_names_to_entrez_ids = {}\n",
    "\n",
    "dna_identifier_gene_name_entrez = pd.read_csv('data_processed/dnaexome_identifier_gene_name_entrez_id.csv')\n",
    "# for each gene name in \"Gene name (d)\" column, if it is not in the dictionary, add it with the entrez ID in \"Entrez gene id (e)\" column\n",
    "for index, row in dna_identifier_gene_name_entrez.iterrows():\n",
    "    gene_name = row['Gene name (d)']\n",
    "    entrez_id = row['Entrez gene id (e)']\n",
    "    if gene_name not in gene_names_to_entrez_ids:\n",
    "        gene_names_to_entrez_ids[gene_name] = entrez_id\n",
    "    else:\n",
    "        # Check if the entrez id is the same\n",
    "        if gene_names_to_entrez_ids[gene_name] != entrez_id:\n",
    "            print('Error: gene name ' + str(gene_name) + ' has two different entrez IDs: ' + str(gene_names_to_entrez_ids[gene_name]) + ' and ' + str(entrez_id))\n",
    "\n",
    "rna_gene_name_entrez = pd.read_csv('data_processed/rna_gene_name_entrez_id.csv')\n",
    "# for each gene name in \"Gene name d\" column, if it is not in the dictionary, add it with the entrez ID in \"Entrez gene id e\" column\n",
    "for index, row in rna_gene_name_entrez.iterrows():\n",
    "    gene_name = row['Gene name d']\n",
    "    entrez_id = row['Entrez gene id e']\n",
    "    if gene_name not in gene_names_to_entrez_ids:\n",
    "        gene_names_to_entrez_ids[gene_name] = entrez_id\n",
    "    else:\n",
    "        # Check if the entrez id is the same\n",
    "        if gene_names_to_entrez_ids[gene_name] != entrez_id:\n",
    "            print('Error: gene name ' + str(gene_name) + ' has two different entrez IDs: ' + str(gene_names_to_entrez_ids[gene_name]) + ' and ' + str(entrez_id))\n",
    "\n",
    "protein_identifier_gene_name_entrez = pd.read_csv('data_processed/protein_identifier_gene_name_entrez_id.csv')\n",
    "# for each gene name in \"Gene name d\" column, if it is not in the dictionary, add it with the entrez ID in \"Entrez gene id e\" column\n",
    "for index, row in protein_identifier_gene_name_entrez.iterrows():\n",
    "    gene_name = row['Gene name d']\n",
    "    entrez_id = row['Entrez gene id e']\n",
    "    if gene_name not in gene_names_to_entrez_ids:\n",
    "        gene_names_to_entrez_ids[gene_name] = entrez_id\n",
    "    else:\n",
    "        # Check if the entrez id is the same\n",
    "        if gene_names_to_entrez_ids[gene_name] != entrez_id:\n",
    "            print('Error: gene name ' + str(gene_name) + ' has two different entrez IDs: ' + str(gene_names_to_entrez_ids[gene_name]) + ' and ' + str(entrez_id))\n",
    "\n",
    "# Print how many gene names are mapped to entrez IDs\n",
    "print('Number of gene names mapped to entrez IDs: ' + str(len(gene_names_to_entrez_ids)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Collect all STRING IDs that could not be mapped to either uniprot or from uniprot to gene id (STRING_unmapped_stringids.txt + STRING_unmapped_uniprot_ids.txt)\n",
    "- Read in STRING IDs -> protein symbols list\n",
    "- Convert unmapped STRING IDs -> protein symbols -> entrez_ids\n",
    "- Keep track of any IDs that still cannot be mapped in a separate list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unmapped STRING IDs -> entrez IDs using protein symbols\n",
    "unmapped_STRING_ids = set()\n",
    "\n",
    "with open('data_processed/STRING_unmapped_stringids.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        unmapped_STRING_ids.add(line.strip())\n",
    "print(\"No uniprot IDs from string ID: \" + str(len(unmapped_STRING_ids)))\n",
    "\n",
    "unmapped_uniprot_ids = set()\n",
    "with open('data_processed/STRING_unmapped_uniprot_ids.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        unmapped_uniprot_ids.add(line.strip())\n",
    "print(\"No entrez IDs from uniprot: \" + str(len(unmapped_uniprot_ids)))\n",
    "\n",
    "# Get df with mapping from STRING ID to UniProtKB ID\n",
    "mapped_uniprot = pd.read_csv('data_processed/stringids_to_uniprotkb.tsv', sep='\\t')\n",
    "for id in unmapped_uniprot_ids:\n",
    "    string_id = mapped_uniprot[mapped_uniprot['Entry'] == id]['From'].values[0]\n",
    "    unmapped_STRING_ids.add(string_id)\n",
    "\n",
    "print(\"All unmapped STRING IDs: \" + str(len(unmapped_STRING_ids)))\n",
    "\n",
    "stringid_to_protsym = get_protein_id_to_symbol_dict()\n",
    "\n",
    "# For each unmapped STRING ID, find the corresponding protein symbol, and if it does not exist, add \n",
    "# list of string_ids_no_protsym\n",
    "string_ids_no_protsym = []\n",
    "mapped_string_ids_to_protsym = {}\n",
    "for id in unmapped_STRING_ids:\n",
    "    if id in stringid_to_protsym:\n",
    "        mapped_string_ids_to_protsym[id] = stringid_to_protsym[id]\n",
    "    else:\n",
    "        string_ids_no_protsym.append(id)\n",
    "\n",
    "print(\"No protein symbols: \" + str(len(string_ids_no_protsym))) # 0\n",
    "print(\"Mapped protein symbols: \" + str(len(mapped_string_ids_to_protsym.keys()))) # 911\n",
    "\n",
    "\n",
    "# Map protein symbols to entrez IDs\n",
    "string_no_entrez = []\n",
    "string_via_protsym_to_entrez = {}\n",
    "protsym_derived_entrez = set()\n",
    "example = 0\n",
    "for stringid in mapped_string_ids_to_protsym:\n",
    "    protsym = mapped_string_ids_to_protsym[stringid]\n",
    "    if protsym in gene_names_to_entrez_ids:\n",
    "        entrez_id = gene_names_to_entrez_ids[protsym]\n",
    "        string_via_protsym_to_entrez[stringid] = entrez_id\n",
    "        protsym_derived_entrez.add(entrez_id)\n",
    "        if example < 5: # Verified top 5 examples manually\n",
    "            print(str(stringid) + ' ' + str(protsym) + ' ' + str(entrez_id))\n",
    "            example += 1\n",
    "    else:\n",
    "        string_no_entrez.append(stringid)\n",
    "\n",
    "print(\"No entrez for protein symbols: \" + str(len(string_no_entrez))) # 390\n",
    "print(\"Mapped protein symbols to entrez IDs: \" + str(len(string_via_protsym_to_entrez))) # 521\n",
    "\n",
    "# Save file of string IDs with no entrez ID\n",
    "with open('data_processed/STRING_no_entrez.txt', 'w') as f:\n",
    "    for id in string_no_entrez:\n",
    "        f.write(id + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several uniprot IDs that map to multiple entrez IDs, but we can just add all of these to one big list of entrez IDs, store a mapping of the entrez ID to STRING ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_mapped_gene = pd.read_csv('data_processed/uniprot_ids_to_gene_ids.tsv', sep='\\t')\n",
    "\n",
    "# Figure out which uniprot IDs are mapped to multiple gene IDs\n",
    "uniprot_ids_to_gene_ids = {}\n",
    "for index, row in uniprot_mapped_gene.iterrows():\n",
    "    uniprot_id = row['From']\n",
    "    gene_id = row['To']\n",
    "    if uniprot_id not in uniprot_ids_to_gene_ids:\n",
    "        uniprot_ids_to_gene_ids[uniprot_id] = [gene_id]\n",
    "    else:\n",
    "        uniprot_ids_to_gene_ids[uniprot_id].append(gene_id)\n",
    "\n",
    "# Which uniprot IDs are mapped to multiple gene IDs\n",
    "uniprots_with_multiple_gene_ids = []\n",
    "for uniprot_id in uniprot_ids_to_gene_ids:\n",
    "    if len(uniprot_ids_to_gene_ids[uniprot_id]) > 1:\n",
    "        uniprots_with_multiple_gene_ids.append(uniprot_id)\n",
    "print('Number of uniprot IDs mapped to multiple gene IDs: ', len(uniprots_with_multiple_gene_ids))\n",
    "# This is okay because from protein expression or any other data, we don't mind connecting multiple\n",
    "# entrez IDs to one STRING protein node in the STRING layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all STRING IDs to Entrez IDs, create dataframe with columns STRING ID, Protein Symbol, Uniprot ID, Entrez ID, save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mapped_string_ids_to_entrez = pd.DataFrame(columns=['STRING_ID', 'Protein_Symbol', 'Uniprot_ID', 'Entrez_Gene_ID'])\n",
    "stringids_no_entrez = []\n",
    "for stringid in known_STRING_ids:\n",
    "    protein_symbol = stringid_to_protsym[stringid]\n",
    "    entrez_id = ''\n",
    "    uniprot_id = '-'\n",
    "    if stringid in mapped_uniprot['From'].values:\n",
    "        # get the row index of where stringid is in the From column\n",
    "        row_index = mapped_uniprot[mapped_uniprot['From'] == stringid].index[0]\n",
    "        uniprot_id = mapped_uniprot['Entry'][row_index]\n",
    "    \n",
    "    if stringid in string_via_protsym_to_entrez:\n",
    "        entrez_id = string_via_protsym_to_entrez[stringid]\n",
    "        new_row_dict = {'STRING_ID': stringid, 'Protein_Symbol': protein_symbol, 'Uniprot_ID': uniprot_id, 'Entrez_Gene_ID': entrez_id}\n",
    "        all_mapped_string_ids_to_entrez.loc[len(all_mapped_string_ids_to_entrez)] = new_row_dict\n",
    "    elif uniprot_id in uniprot_ids_to_gene_ids:\n",
    "        for gene_id in uniprot_ids_to_gene_ids[uniprot_id]:\n",
    "            new_row_dict = {'STRING_ID': stringid, 'Protein_Symbol': protein_symbol, 'Uniprot_ID': uniprot_id, 'Entrez_Gene_ID': gene_id}\n",
    "            all_mapped_string_ids_to_entrez.loc[len(all_mapped_string_ids_to_entrez)] = new_row_dict\n",
    "    else:\n",
    "        stringids_no_entrez.append(stringid)\n",
    "    \n",
    "print('Number of known STRING IDs not mapped to entrez IDs: ' + str(len(stringids_no_entrez)))\n",
    "print('Number of mapped STRING IDs to entrez IDs (including multiple entrez IDs for the same STRING): ' + str(len(all_mapped_string_ids_to_entrez)))\n",
    "print('Number of unique STRING IDs that are mapped to entrez IDs: ' + str(len(all_mapped_string_ids_to_entrez['STRING_ID'].unique())))\n",
    "print(all_mapped_string_ids_to_entrez.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to file\n",
    "all_mapped_string_ids_to_entrez.to_csv('data_processed/string_ids_prot_entrez.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to preprocess the NCI-ALMANAC dataset to include HSA and ZIP scores from DrugComb, code for this is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "almanac_df = pd.read_csv('data/NCI-ALMANAC/ComboDrugGrowth_Nov2017.csv')\n",
    "almanac_df = almanac_df.dropna(subset=['NSC1', 'NSC2', 'CONC1', 'CONC2', 'CONCUNIT1', 'CONCUNIT2', 'CELLNAME', 'PERCENTGROWTH', 'PANEL', 'SCORE'])\n",
    "almanac_df['NSC1'] = almanac_df['NSC1'].apply(lambda x: int(x))\n",
    "almanac_df['NSC2'] = almanac_df['NSC2'].apply(lambda x: int(x))\n",
    "\n",
    "# Create file of cell line name to ID mapping\n",
    "cl_names = almanac_df[\"CELLNAME\"].unique()\n",
    "# zip together the cell line names and a range of numbers\n",
    "almanac_cl_name_to_id = dict(zip(cl_names, range(len(cl_names))))\n",
    "\n",
    "# Write the cell line name to ID mapping to a file\n",
    "with open(\"data_processed/almanac_cell_line_to_id.csv\", \"w\") as f:\n",
    "    f.write(\"Cell Line Name,Cell Line ID\\n\")\n",
    "    for key in sorted(almanac_cl_name_to_id.keys()):\n",
    "        f.write(\"%s,%s\\n\" % (key, almanac_cl_name_to_id[key]))\n",
    "\n",
    "drugcomb_df = pd.read_csv('data/DrugComb/drugcomb_summary_v_1_5.csv')\n",
    "drugcomb_df = drugcomb_df[drugcomb_df['study_name'] == 'ALMANAC']\n",
    "drugcomb_df = drugcomb_df.dropna(subset=['drug_row', 'drug_col', 'cell_line_name', 'synergy_zip', 'synergy_hsa'])\n",
    "\n",
    "names_to_nsc = {}\n",
    "with open('data/NCI-ALMANAC/ComboCompoundNames_all.txt') as f: # This file had some issues with the formatting, may have to replace the last four tabs manually\n",
    "    for line in f:\n",
    "        entries = line.split('\\t')\n",
    "        names_to_nsc[entries[1].strip('\\n').lower()] = int(entries[0])\n",
    "\n",
    "manual_drug_names_to_nsc = { # Found missing names to NSCs via PubChem/Google Searches\n",
    "    'sunitinib': 750690,\n",
    "    'cis-platin': 119875,\n",
    "    'nsc707389': 707389,\n",
    "    'navelbine ditartrate (tn)': 608210,\n",
    "    'eloxatin (tn) (sanofi synthelab)': 266046,\n",
    "    'nsc256439': 256439,\n",
    "    'fludarabine base': 118218,\n",
    "    'nsc733504': 733504,\n",
    "    'nsc-127716': 127716,\n",
    "    '5-aminolevulinic acid hydrochloride': 18509,\n",
    "    'chembl277800': 69,\n",
    "    'emcyt (pharmacia)': 702294,\n",
    "    'carboplatinum': 241240, # carboplatin\n",
    "    'chembl17639':105014,\n",
    "    '23541-50-6': 82151, # daunorubicin\n",
    "    '55-86-7': 762, # mechlorethamine\n",
    "    '7803-88-5': 66381, #6'-O-Methylguanosine\n",
    "    '158798-73-3': 712807, # capecitabine\n",
    "    '122111-05-1': 613327, # gemcitabine\n",
    "}\n",
    "names_to_nsc.update(manual_drug_names_to_nsc)\n",
    "\n",
    "# Map all drug_row and drug_col names to NSC numbers\n",
    "drugcomb_df['drug_row'] = drugcomb_df['drug_row'].apply(lambda x: names_to_nsc[x.lower()])\n",
    "drugcomb_df['drug_col'] = drugcomb_df['drug_col'].apply(lambda x: names_to_nsc[x.lower()])\n",
    "\n",
    "# What is the shape of the remaining DrugComb data?\n",
    "print(\"Filtered DrugComb: \", drugcomb_df.shape)\n",
    "\n",
    "almanac_cell_lines_to_drugcomb = { # Manually map some NCI-ALMANAC cell line names to DrugComb cell line names\n",
    "    'A549/ATCC': 'A549',\n",
    "    'NCI-H23': 'NCIH23',\n",
    "    'MDA-MB-231/ATCC': 'MDA-MB-231',\n",
    "    'UACC-62': 'UACC62',\n",
    "    'HCT-116': 'HCT116',\n",
    "    'OVCAR-3': 'OVCAR3',\n",
    "}\n",
    "\n",
    "almanac_df['CELLNAME'] = almanac_df['CELLNAME'].apply(lambda x: almanac_cell_lines_to_drugcomb[x] if x in almanac_cell_lines_to_drugcomb else x)\n",
    "almanac_df = almanac_df[almanac_df['CELLNAME'] != 'SF-539\\x1a'] # There is no DrugComb cell line data for 'SF-539\\x1a', so we drop it\n",
    "almanac_concentrations_df = almanac_df[['NSC1', 'CONC1', 'NSC2', 'CONC2', 'CELLNAME', 'PANEL', 'PERCENTGROWTH']]\n",
    "almanac_scores_df = almanac_df.groupby(['NSC1', 'NSC2', 'CELLNAME', 'PANEL'], as_index=False)['SCORE'].sum().rename(columns={'SCORE': 'COMBOSCORE'})\n",
    "\n",
    "# What is the shape of the remaining NCI-ALMANAC for the percent growth data?\n",
    "print(\"With Conc, Filtered NCI-ALMANAC: \", almanac_concentrations_df.shape)\n",
    "\n",
    "# What is the shape of the ComboScore NCI-ALMANAC data?\n",
    "print(\"ComboScored, Filtered NCI-ALMANAC: \", almanac_scores_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZIP and HSA synergy scores are not always order invariant to which drug is used in drug_row vs drug_col\n",
    "# Get unique NSC1, NSC2, CELLNAME pairs from almanac_df\n",
    "unique_triplicates = almanac_scores_df[['NSC1', 'NSC2', 'CELLNAME']].drop_duplicates().reset_index(drop=True)\n",
    "print('Number of unique NSC1, NSC2, CELLNAME pairs:', len(unique_triplicates))\n",
    "\n",
    "# Create dictionaries for fast lookups\n",
    "first_way_dict = defaultdict(list)  # For (drug_row, drug_col, cell_line)\n",
    "second_way_dict = defaultdict(list) # For (drug_col, drug_row, cell_line)\n",
    "\n",
    "# Populate the dictionaries in a single pass through drugcomb_df\n",
    "for _, row in drugcomb_df.iterrows():\n",
    "    drug_row = row['drug_row']\n",
    "    drug_col = row['drug_col']\n",
    "    cell_line = row['cell_line_name']\n",
    "    zip_score = row['synergy_zip']\n",
    "    hsa_score = row['synergy_hsa']\n",
    "    \n",
    "    # Add to first way dictionary\n",
    "    first_way_dict[(drug_row, drug_col, cell_line)].append((zip_score, hsa_score))\n",
    "    \n",
    "    # Add to second way dictionary (for the flipped case)\n",
    "    second_way_dict[(drug_col, drug_row, cell_line)].append((zip_score, hsa_score))\n",
    "\n",
    "# Initialize empty columns\n",
    "unique_triplicates['First_way_ZIP'] = np.nan\n",
    "unique_triplicates['First_way_HSA'] = np.nan\n",
    "unique_triplicates['Second_way_ZIP'] = np.nan\n",
    "unique_triplicates['Second_way_HSA'] = np.nan\n",
    "\n",
    "# Track multiple scores\n",
    "multiple_matching_score_triplicates = set()\n",
    "multiple_flipped_matching_score_triplicates = set()\n",
    "\n",
    "# Process all rows efficiently using the dictionaries\n",
    "for idx, row in unique_triplicates.iterrows():\n",
    "    drug_row = row['NSC1']\n",
    "    drug_col = row['NSC2']\n",
    "    cell_line = row['CELLNAME']\n",
    "    \n",
    "    # Check first way\n",
    "    first_way_matches = first_way_dict.get((drug_row, drug_col, cell_line), [])\n",
    "    if len(first_way_matches) > 1:\n",
    "        multiple_matching_score_triplicates.add((drug_row, drug_col, cell_line))\n",
    "    elif len(first_way_matches) == 1:\n",
    "        zip_score, hsa_score = first_way_matches[0]\n",
    "        unique_triplicates.at[idx, 'First_way_ZIP'] = zip_score\n",
    "        unique_triplicates.at[idx, 'First_way_HSA'] = hsa_score\n",
    "    \n",
    "    # Check second way (flipped)\n",
    "    second_way_matches = second_way_dict.get((drug_row, drug_col, cell_line), [])\n",
    "    if len(second_way_matches) > 1:\n",
    "        multiple_flipped_matching_score_triplicates.add((drug_row, drug_col, cell_line))\n",
    "    elif len(second_way_matches) == 1:\n",
    "        zip_score, hsa_score = second_way_matches[0]\n",
    "        unique_triplicates.at[idx, 'Second_way_ZIP'] = zip_score\n",
    "        unique_triplicates.at[idx, 'Second_way_HSA'] = hsa_score\n",
    "\n",
    "# Print statistics\n",
    "print('Number of entries with multiple matching scores:', len(multiple_matching_score_triplicates))\n",
    "print('Number of entries with multiple flipped matching scores:', len(multiple_flipped_matching_score_triplicates))\n",
    "\n",
    "# Retrieve the unique entries that only have one ZIP score and one HSA score\n",
    "unique_singleziphsa_entries = unique_triplicates[\n",
    "    (unique_triplicates['First_way_ZIP'].notna() ^ unique_triplicates['Second_way_ZIP'].notna()) &\n",
    "    (unique_triplicates['First_way_HSA'].notna() ^ unique_triplicates['Second_way_HSA'].notna())\n",
    "]\n",
    "\n",
    "unique_singleziphsa_entries = unique_triplicates[\n",
    "    (unique_triplicates['First_way_ZIP'].notna() ^ unique_triplicates['Second_way_ZIP'].notna()) &\n",
    "    (unique_triplicates['First_way_HSA'].notna() ^ unique_triplicates['Second_way_HSA'].notna())\n",
    "].copy()\n",
    "\n",
    "unique_singleziphsa_entries.loc[:, 'ZIP'] = unique_singleziphsa_entries.apply(\n",
    "    lambda row: row['First_way_ZIP'] if pd.notna(row['First_way_ZIP']) else row['Second_way_ZIP'], axis=1\n",
    ")\n",
    "\n",
    "unique_singleziphsa_entries.loc[:, 'HSA'] = unique_singleziphsa_entries.apply(\n",
    "    lambda row: row['First_way_HSA'] if pd.notna(row['First_way_HSA']) else row['Second_way_HSA'], axis=1\n",
    ")\n",
    "unique_singleziphsa_entries = unique_singleziphsa_entries.drop(columns=['First_way_ZIP', 'First_way_HSA', 'Second_way_ZIP', 'Second_way_HSA'])\n",
    "print('Number of unique entries with one ZIP score and one HSA score:', len(unique_singleziphsa_entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the almanac_scores_df with the unique_singleziphsa_entries after the rightmost column\n",
    "merged_scores_df = pd.merge(\n",
    "    unique_singleziphsa_entries,\n",
    "    almanac_scores_df,\n",
    "    how='right',\n",
    "    left_on=['NSC1', 'NSC2', 'CELLNAME'],\n",
    "    right_on=['NSC1', 'NSC2', 'CELLNAME'],\n",
    "    suffixes=('', '_y')\n",
    ")\n",
    "# Drop the extra columns from the right DataFrame\n",
    "merged_scores_df = merged_scores_df.loc[:, ~merged_scores_df.columns.str.endswith('_y')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the concentration dataframe to a CSV file\n",
    "almanac_concentrations_df.to_csv('data_processed/almcomb_concentrations.csv', index=False)\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "merged_scores_df.to_csv('data_processed/almcomb_comboscore_hsa_zip.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer a few questions about the drug data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many drug-pairs remain after all the preprocessing?\n",
    "print(\"ALMANAC Concentrations, Number of Unique Drug Pairs: \", len(almanac_concentrations_df[['NSC1', 'NSC2']].drop_duplicates()))\n",
    "print(\"ALMANAC ComboScores, Number of Unique Drug Pairs: \", len(merged_scores_df[['NSC1', 'NSC2']].drop_duplicates()))\n",
    "\n",
    "# How many triples (NSC1, NSC2, CELLNAME) remain after all the preprocessing?\n",
    "print(\"ALMANAC Concentrations, Number of Unique Triplicates: \", len(almanac_concentrations_df[['NSC1', 'NSC2', 'CELLNAME']].drop_duplicates()))\n",
    "print(\"ALMANAC ComboScores, Number of Unique Triplicates: \", len(merged_scores_df[['NSC1', 'NSC2', 'CELLNAME']].drop_duplicates()))\n",
    "\n",
    "# How many triples (NSC1, NSC2, CELLNAME) agree on synergistic/additive/antagonistic?\n",
    "synergy_zip = merged_scores_df['ZIP'].apply(lambda x: 'synergistic' if x > 0 else ('antagonistic' if x < 0 else 'additive'))\n",
    "synergy_hsa = merged_scores_df['HSA'].apply(lambda x: 'synergistic' if x > 0 else ('antagonistic' if x < 0 else 'additive'))\n",
    "synergy_comboscore = merged_scores_df['COMBOSCORE'].apply(lambda x: 'synergistic' if x > 0 else ('antagonistic' if x < 0 else 'additive'))\n",
    "agreed_categories = pd.DataFrame({\n",
    "    'synergy_zip': synergy_zip,\n",
    "    'synergy_hsa': synergy_hsa,\n",
    "    'synergy_comboscore': synergy_comboscore\n",
    "})\n",
    "agreed_categories['agreed'] = agreed_categories.apply(\n",
    "    lambda row: 1 if row['synergy_zip'] == row['synergy_hsa'] == row['synergy_comboscore'] else 0,\n",
    "    axis=1\n",
    ")\n",
    "agreed_count = agreed_categories['agreed'].sum()\n",
    "disagreed_count = len(agreed_categories) - agreed_count\n",
    "print(\"Number of triples that agree on synergistic/additive/antagonistic: \", agreed_count)\n",
    "print(\"Number of triples that disagree on synergistic/additive/antagonistic: \", disagreed_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Histogram of Cell Lines frequency in the merged_scores_df\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.histplot(merged_scores_df['CELLNAME'], bins=30, kde=True)\n",
    "plt.title('Cell Lines Frequency in Combined Concentration Synergy Score Dataset')\n",
    "plt.xlabel('Cell Line')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/supplemental_cell_line_histogram_combinedconc.png')\n",
    "\n",
    "# Print the mean, median, and standard deviation of the frequency of cell lines\n",
    "cell_line_counts = merged_scores_df['CELLNAME'].value_counts()\n",
    "mean_count = cell_line_counts.mean()\n",
    "median_count = cell_line_counts.median()\n",
    "std_count = cell_line_counts.std()\n",
    "print(f'Mean count of cell lines: {mean_count}')\n",
    "print(f'Median count of cell lines: {median_count}')\n",
    "print(f'Standard deviation of cell lines: {std_count}')\n",
    "# Print the top 5 cell lines with the highest frequency\n",
    "print('Top 5 cell lines with the highest frequency:')\n",
    "print(cell_line_counts.head(5))\n",
    "# Print the top 5 cell lines with the lowest frequency\n",
    "print('Top 5 cell lines with the lowest frequency:')\n",
    "print(cell_line_counts.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Cell Lines frequency in the merged_scores_df\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.histplot(almanac_concentrations_df['CELLNAME'], bins=30, kde=True)\n",
    "plt.title('Cell Lines Frequency in Concentration Dependent Synergy Score Dataset')\n",
    "plt.xlabel('Cell Line')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/supplemental_cell_line_histogram_conc.png')\n",
    "\n",
    "# Print the mean, median, and standard deviation of the frequency of cell lines\n",
    "cell_line_counts = almanac_concentrations_df['CELLNAME'].value_counts()\n",
    "mean_count = cell_line_counts.mean()\n",
    "median_count = cell_line_counts.median()\n",
    "std_count = cell_line_counts.std()\n",
    "print(f'Mean count of cell lines: {mean_count}')\n",
    "print(f'Median count of cell lines: {median_count}')\n",
    "print(f'Standard deviation of cell lines: {std_count}')\n",
    "# Print the top 5 cell lines with the highest frequency\n",
    "print('Top 5 cell lines with the highest frequency:')\n",
    "print(cell_line_counts.head(5))\n",
    "# Print the top 5 cell lines with the lowest frequency\n",
    "print('Top 5 cell lines with the lowest frequency:')\n",
    "print(cell_line_counts.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each unique drug in the merged_scores_df, add the number of times it appears in the dataset\n",
    "unique_drugs_merged = set(merged_scores_df['NSC1']).union(set(merged_scores_df['NSC2']))\n",
    "nsc_to_drug_name = {}\n",
    "with open('data/NCI-ALMANAC/ComboCompoundNames_small.txt') as f:\n",
    "    for line in f:\n",
    "        entries = line.split('\\t')\n",
    "        nsc_to_drug_name[int(entries[0])] = entries[1].strip('\\n').lower()\n",
    "\n",
    "# Check if multiple NSC IDs map to the same drug name\n",
    "drug_name_to_nscs = {}\n",
    "for nsc, drug_name in nsc_to_drug_name.items():\n",
    "    if drug_name in drug_name_to_nscs:\n",
    "        drug_name_to_nscs[drug_name].append(nsc)\n",
    "    else:\n",
    "        drug_name_to_nscs[drug_name] = [nsc]\n",
    "\n",
    "# Print all drug names and their associated NSC IDs\n",
    "print(\"Mapping of drug names to NSC IDs:\")\n",
    "for drug_name, nscs in sorted(drug_name_to_nscs.items()):\n",
    "    if len(nscs) > 1:\n",
    "        print(f\"  {drug_name}: Multiple NSC IDs {nscs}\")\n",
    "\n",
    "# Initialize counts for each drug\n",
    "drug_name_to_counts = {drug_name: 0 for drug_name in drug_name_to_nscs.keys()}\n",
    "\n",
    "# Count occurrences for each NSC ID and add to the corresponding drug name\n",
    "for drug in unique_drugs_merged:\n",
    "    drug_name = nsc_to_drug_name[drug]\n",
    "    drug_count = len(merged_scores_df[(merged_scores_df['NSC1'] == drug) | (merged_scores_df['NSC2'] == drug)])\n",
    "    drug_name_to_counts[drug_name] += drug_count\n",
    "\n",
    "# Print counts for drugs with multiple NSC IDs\n",
    "print(\"\\nCounts for drugs with multiple NSC IDs:\")\n",
    "for drug_name, nscs in drug_name_to_nscs.items():\n",
    "    if len(nscs) > 1:\n",
    "        # Print individual NSC counts\n",
    "        total_count = 0\n",
    "        for nsc in nscs:\n",
    "            nsc_count = len(merged_scores_df[(merged_scores_df['NSC1'] == nsc) | (merged_scores_df['NSC2'] == nsc)])\n",
    "            print(f\"  {drug_name} - NSC {nsc}: {nsc_count} occurrences\")\n",
    "            total_count += nsc_count\n",
    "        # Print combined count\n",
    "        print(f\"  TOTAL for {drug_name}: {total_count} occurrences\\n\")\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "drug_counts_df = pd.DataFrame({\n",
    "    'Drug_Name': list(drug_name_to_counts.keys()),\n",
    "    'Count': list(drug_name_to_counts.values())\n",
    "})\n",
    "\n",
    "# Sort alphabetically by drug name\n",
    "drug_counts_df = drug_counts_df.sort_values('Drug_Name')\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x='Drug_Name', y='Count', data=drug_counts_df)\n",
    "plt.title('Drug Names Frequency in Combined Concentration Synergy Score Dataset')\n",
    "plt.xlabel('Drug Name')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/supplemental_drug_histogram_combinedconc.png')\n",
    "\n",
    "# Print the mean, median, and standard deviation of the frequency of drugs\n",
    "mean_count = drug_counts_df['Count'].mean()\n",
    "median_count = drug_counts_df['Count'].median()\n",
    "std_count = drug_counts_df['Count'].std()\n",
    "print(f'Mean count of drugs: {mean_count}')\n",
    "print(f'Median count of drugs: {median_count}')\n",
    "print(f'Standard deviation of drugs: {std_count}')\n",
    "# Print the top 5 drugs with the highest frequency\n",
    "print('Top 5 drugs with the highest frequency:')\n",
    "print(drug_counts_df.sort_values('Count', ascending=False).head(5))\n",
    "# Print the top 5 drugs with the lowest frequency\n",
    "print('Top 5 drugs with the lowest frequency:')\n",
    "print(drug_counts_df.sort_values('Count', ascending=True).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each unique drug in the almanac_concentrations_df, add the number of times it appears in the dataset\n",
    "unique_drugs_almanac = set(almanac_concentrations_df['NSC1']).union(set(almanac_concentrations_df['NSC2']))\n",
    "# Initialize counts for each drug\n",
    "drug_name_to_counts_almanac = {drug_name: 0 for drug_name in drug_name_to_nscs.keys()}\n",
    "# Count occurrences for each NSC ID and add to the corresponding drug name\n",
    "for drug in unique_drugs_almanac:\n",
    "    drug_name = nsc_to_drug_name[drug]\n",
    "    drug_count = len(almanac_concentrations_df[(almanac_concentrations_df['NSC1'] == drug) | (almanac_concentrations_df['NSC2'] == drug)])\n",
    "    drug_name_to_counts_almanac[drug_name] += drug_count\n",
    "# Convert dictionary to DataFrame\n",
    "drug_counts_df_almanac = pd.DataFrame({\n",
    "    'Drug_Name': list(drug_name_to_counts_almanac.keys()),\n",
    "    'Count': list(drug_name_to_counts_almanac.values())\n",
    "})\n",
    "# Sort alphabetically by drug name\n",
    "drug_counts_df_almanac = drug_counts_df_almanac.sort_values('Drug_Name')\n",
    "# Create the plot\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x='Drug_Name', y='Count', data=drug_counts_df_almanac)\n",
    "plt.title('Drug Names Frequency in Concentration Dependent Synergy Score Dataset')\n",
    "plt.xlabel('Drug Name')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/supplemental_drug_histogram_conc.png')\n",
    "# Print the mean, median, and standard deviation of the frequency of drugs\n",
    "mean_count = drug_counts_df_almanac['Count'].mean()\n",
    "median_count = drug_counts_df_almanac['Count'].median()\n",
    "std_count = drug_counts_df_almanac['Count'].std()\n",
    "print(f'Mean count of drugs: {mean_count}')\n",
    "print(f'Median count of drugs: {median_count}')\n",
    "print(f'Standard deviation of drugs: {std_count}')\n",
    "# Print the top 5 drugs with the highest frequency\n",
    "print('Top 5 drugs with the highest frequency:')\n",
    "print(drug_counts_df_almanac.sort_values('Count', ascending=False).head(5))\n",
    "# Print the top 5 drugs with the lowest frequency\n",
    "print('Top 5 drugs with the lowest frequency:')\n",
    "print(drug_counts_df_almanac.sort_values('Count', ascending=True).head(5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
