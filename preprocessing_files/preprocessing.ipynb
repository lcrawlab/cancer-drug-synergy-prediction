{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the data #\n",
    "\n",
    "Pre-requisites to using this file\n",
    "- Run the string_preprocessing.py main function and store the STRING database with known interactions as ../data_processed/STRING_full_filtered.tsv\n",
    "- Manually retrieve STRING IDs to UniProtIDs and UniProtIDs to GeneIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from preprocess_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the DNA, RNA, and Protein data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (140258, 76)\n",
      "Transposed shape: (60, 138892)\n",
      "Original shape: (26188, 66)\n",
      "Transposed shape: (60, 26178)\n",
      "Original shape: (3181, 69)\n",
      "Transposed shape: (59, 3167)\n"
     ]
    }
   ],
   "source": [
    "preprocess_dna_data()\n",
    "preprocess_rna_data()\n",
    "preprocess_protein_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the string_preprocessing.py main function before this!\n",
    "\n",
    "Get all of the STRING IDs from the full filtered STRING and save to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of known STRING IDs: 18727\n"
     ]
    }
   ],
   "source": [
    "known_STRING = pd.read_csv('../data_processed/STRING_full_filtered.tsv', sep='\\t')\n",
    "# Get all STRING IDs in both columns protein1 and protein2\n",
    "known_STRING_ids = set(known_STRING['protein1']).union(set(known_STRING['protein2']))\n",
    "# Save all STRING IDs in a file\n",
    "with open('../data_processed/known_STRING_ids.txt', 'w') as f:\n",
    "    for id in known_STRING_ids:\n",
    "        f.write(id + '\\n')\n",
    "\n",
    "print('Number of known STRING IDs: {}'.format(len(known_STRING_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually get the STRING id to UniProt ID conversion from https://www.uniprot.org/id-mapping/ ###\n",
    "\n",
    "Manually retrieved STRING IDs to UniProtKB -- 18,305 IDs were mapped to 18,305 results, but 422 ID were not mapped\n",
    "- Mapped STRING IDs saved at \"../data_processed/stringids_to_uniprotkb.tsv\"\n",
    "- Unmapped STRING IDs saved at \"../data_processed/STRING_unmapped_stringids.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_uniprot = pd.read_csv('../data_processed/stringids_to_uniprotkb.tsv', sep='\\t')\n",
    "# Get all UniProtKB IDs in \"Entry\" column\n",
    "mapped_uniprot_ids = set(mapped_uniprot['Entry'])\n",
    "# Save all UniProtKB IDs in a file\n",
    "with open('../data_processed/STRING_mapped_uniprot_ids.txt', 'w') as f:\n",
    "    for id in mapped_uniprot_ids:\n",
    "        f.write(id + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually get the UniProt ID conversion to GeneID from https://www.uniprot.org/id-mapping/ ###\n",
    "\n",
    "UniProtKB AC/ID to GeneID -- 17,816 IDs were mapped to 18,018 results, but 489 ID were not mapped\n",
    "- Mapped UniProtIDs saved at \"../data_processed/uniprot_ids_to_gene_id.tsv\"\n",
    "- Unmapped UniProtIDs saved at \"../data_processed/STRING_unmapped_uniprotids.txt\"\n",
    "\n",
    "The mapped gene IDs have significant intersection with the DNA/RNA/Protein intersection: found 2638 out of 2667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the entrez IDs for the IDs that couldn't be mapped. Go from STRING ID -> Protein Symbol (assume same as gene name) -> Entrez ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of gene names mapped to entrez IDs: 27163\n"
     ]
    }
   ],
   "source": [
    "# Get all gene names to entrez IDs possible\n",
    "gene_names_to_entrez_ids = {}\n",
    "\n",
    "dna_identifier_gene_name_entrez = pd.read_csv('../data_processed/dnaexome_identifier_gene_name_entrez_id.csv')\n",
    "# for each gene name in \"Gene name (d)\" column, if it is not in the dictionary, add it with the entrez ID in \"Entrez gene id (e)\" column\n",
    "for index, row in dna_identifier_gene_name_entrez.iterrows():\n",
    "    gene_name = row['Gene name (d)']\n",
    "    entrez_id = row['Entrez gene id (e)']\n",
    "    if gene_name not in gene_names_to_entrez_ids:\n",
    "        gene_names_to_entrez_ids[gene_name] = entrez_id\n",
    "    else:\n",
    "        # Check if the entrez id is the same\n",
    "        if gene_names_to_entrez_ids[gene_name] != entrez_id:\n",
    "            print('Error: gene name ' + str(gene_name) + ' has two different entrez IDs: ' + str(gene_names_to_entrez_ids[gene_name]) + ' and ' + str(entrez_id))\n",
    "\n",
    "rna_gene_name_entrez = pd.read_csv('../data_processed/rna_gene_name_entrez_id.csv')\n",
    "# for each gene name in \"Gene name d\" column, if it is not in the dictionary, add it with the entrez ID in \"Entrez gene id e\" column\n",
    "for index, row in rna_gene_name_entrez.iterrows():\n",
    "    gene_name = row['Gene name d']\n",
    "    entrez_id = row['Entrez gene id e']\n",
    "    if gene_name not in gene_names_to_entrez_ids:\n",
    "        gene_names_to_entrez_ids[gene_name] = entrez_id\n",
    "    else:\n",
    "        # Check if the entrez id is the same\n",
    "        if gene_names_to_entrez_ids[gene_name] != entrez_id:\n",
    "            print('Error: gene name ' + str(gene_name) + ' has two different entrez IDs: ' + str(gene_names_to_entrez_ids[gene_name]) + ' and ' + str(entrez_id))\n",
    "\n",
    "protein_identifier_gene_name_entrez = pd.read_csv('../data_processed/protein_identifier_gene_name_entrez_id.csv')\n",
    "# for each gene name in \"Gene name d\" column, if it is not in the dictionary, add it with the entrez ID in \"Entrez gene id e\" column\n",
    "for index, row in protein_identifier_gene_name_entrez.iterrows():\n",
    "    gene_name = row['Gene name d']\n",
    "    entrez_id = row['Entrez gene id e']\n",
    "    if gene_name not in gene_names_to_entrez_ids:\n",
    "        gene_names_to_entrez_ids[gene_name] = entrez_id\n",
    "    else:\n",
    "        # Check if the entrez id is the same\n",
    "        if gene_names_to_entrez_ids[gene_name] != entrez_id:\n",
    "            print('Error: gene name ' + str(gene_name) + ' has two different entrez IDs: ' + str(gene_names_to_entrez_ids[gene_name]) + ' and ' + str(entrez_id))\n",
    "\n",
    "# Print how many gene names are mapped to entrez IDs\n",
    "print('Number of gene names mapped to entrez IDs: ' + str(len(gene_names_to_entrez_ids)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Collect all STRING IDs that could not be mapped to either uniprot or from uniprot to gene id (STRING_unmapped_stringids.txt + STRING_unmapped_uniprot_ids.txt)\n",
    "- Read in STRING IDs -> protein symbols list\n",
    "- Convert unmapped STRING IDs -> protein symbols -> entrez_ids\n",
    "- Keep track of any IDs that still cannot be mapped in a separate list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No uniprot IDs from string ID: 422\n",
      "No entrez IDs from uniprot: 489\n",
      "All unmapped STRING IDs: 911\n",
      "No protein symbols: 0\n",
      "Mapped protein symbols: 911\n",
      "9606.ENSP00000345151 RFPL4AL1 729974\n",
      "9606.ENSP00000387888 UGT2A1 10941\n",
      "9606.ENSP00000350392 C17orf102 400591\n",
      "9606.ENSP00000471224 DMRTC1 63947\n",
      "9606.ENSP00000481357 CCL23 6368\n",
      "No entrez for protein symbols: 390\n",
      "Mapped protein symbols to entrez IDs: 521\n"
     ]
    }
   ],
   "source": [
    "# Find unmapped STRING IDs -> entrez IDs using protein symbols\n",
    "unmapped_STRING_ids = set()\n",
    "\n",
    "with open('../data_processed/STRING_unmapped_stringids.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        unmapped_STRING_ids.add(line.strip())\n",
    "print(\"No uniprot IDs from string ID: \" + str(len(unmapped_STRING_ids)))\n",
    "\n",
    "unmapped_uniprot_ids = set()\n",
    "with open('../data_processed/STRING_unmapped_uniprot_ids.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        unmapped_uniprot_ids.add(line.strip())\n",
    "print(\"No entrez IDs from uniprot: \" + str(len(unmapped_uniprot_ids)))\n",
    "\n",
    "# Get df with mapping from STRING ID to UniProtKB ID\n",
    "mapped_uniprot = pd.read_csv('../data_processed/stringids_to_uniprotkb.tsv', sep='\\t')\n",
    "for id in unmapped_uniprot_ids:\n",
    "    string_id = mapped_uniprot[mapped_uniprot['Entry'] == id]['From'].values[0]\n",
    "    unmapped_STRING_ids.add(string_id)\n",
    "\n",
    "print(\"All unmapped STRING IDs: \" + str(len(unmapped_STRING_ids)))\n",
    "\n",
    "stringid_to_protsym = get_protein_id_to_symbol_dict()\n",
    "\n",
    "# For each unmapped STRING ID, find the corresponding protein symbol, and if it does not exist, add \n",
    "# list of string_ids_no_protsym\n",
    "string_ids_no_protsym = []\n",
    "mapped_string_ids_to_protsym = {}\n",
    "for id in unmapped_STRING_ids:\n",
    "    if id in stringid_to_protsym:\n",
    "        mapped_string_ids_to_protsym[id] = stringid_to_protsym[id]\n",
    "    else:\n",
    "        string_ids_no_protsym.append(id)\n",
    "\n",
    "print(\"No protein symbols: \" + str(len(string_ids_no_protsym))) # 0\n",
    "print(\"Mapped protein symbols: \" + str(len(mapped_string_ids_to_protsym.keys()))) # 911\n",
    "\n",
    "\n",
    "# Map protein symbols to entrez IDs\n",
    "string_no_entrez = []\n",
    "string_via_protsym_to_entrez = {}\n",
    "protsym_derived_entrez = set()\n",
    "example = 0\n",
    "for stringid in mapped_string_ids_to_protsym:\n",
    "    protsym = mapped_string_ids_to_protsym[stringid]\n",
    "    if protsym in gene_names_to_entrez_ids:\n",
    "        entrez_id = gene_names_to_entrez_ids[protsym]\n",
    "        string_via_protsym_to_entrez[stringid] = entrez_id\n",
    "        protsym_derived_entrez.add(entrez_id)\n",
    "        if example < 5: # Verified top 5 examples manually\n",
    "            print(str(stringid) + ' ' + str(protsym) + ' ' + str(entrez_id))\n",
    "            example += 1\n",
    "    else:\n",
    "        string_no_entrez.append(stringid)\n",
    "\n",
    "print(\"No entrez for protein symbols: \" + str(len(string_no_entrez))) # 390\n",
    "print(\"Mapped protein symbols to entrez IDs: \" + str(len(string_via_protsym_to_entrez))) # 521\n",
    "\n",
    "# Save file of string IDs with no entrez ID\n",
    "with open('../data_processed/STRING_no_entrez.txt', 'w') as f:\n",
    "    for id in string_no_entrez:\n",
    "        f.write(id + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several uniprot IDs that map to multiple entrez IDs, but we can just add all of these to one big list of entrez IDs, store a mapping of the entrez ID to STRING ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of uniprot IDs mapped to multiple gene IDs:  139\n"
     ]
    }
   ],
   "source": [
    "uniprot_mapped_gene = pd.read_csv('../data_processed/uniprot_ids_to_gene_ids.tsv', sep='\\t')\n",
    "\n",
    "# Figure out which uniprot IDs are mapped to multiple gene IDs\n",
    "uniprot_ids_to_gene_ids = {}\n",
    "for index, row in uniprot_mapped_gene.iterrows():\n",
    "    uniprot_id = row['From']\n",
    "    gene_id = row['To']\n",
    "    if uniprot_id not in uniprot_ids_to_gene_ids:\n",
    "        uniprot_ids_to_gene_ids[uniprot_id] = [gene_id]\n",
    "    else:\n",
    "        uniprot_ids_to_gene_ids[uniprot_id].append(gene_id)\n",
    "\n",
    "# Which uniprot IDs are mapped to multiple gene IDs\n",
    "uniprots_with_multiple_gene_ids = []\n",
    "for uniprot_id in uniprot_ids_to_gene_ids:\n",
    "    if len(uniprot_ids_to_gene_ids[uniprot_id]) > 1:\n",
    "        uniprots_with_multiple_gene_ids.append(uniprot_id)\n",
    "print('Number of uniprot IDs mapped to multiple gene IDs: ', len(uniprots_with_multiple_gene_ids))\n",
    "# This is okay because from protein expression or any other data, we don't mind connecting multiple\n",
    "# entrez IDs to one STRING protein node in the STRING layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all STRING IDs to Entrez IDs, create dataframe with columns STRING ID, Protein Symbol, Uniprot ID, Entrez ID, save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of known STRING IDs not mapped to entrez IDs: 390\n",
      "Number of mapped STRING IDs to entrez IDs (including multiple entrez IDs for the same STRING): 18539\n",
      "Number of unique STRING IDs that are mapped to entrez IDs: 18337\n",
      "              STRING_ID Protein_Symbol Uniprot_ID  Entrez_Gene_ID\n",
      "0  9606.ENSP00000371434           RFX3     P48380            5991\n",
      "1  9606.ENSP00000262662         CDKN2C     P42773            1031\n",
      "2  9606.ENSP00000368245        CXorf21     Q9HAI6           80231\n",
      "3  9606.ENSP00000292672          CELF5     Q8N6W0           60680\n",
      "4  9606.ENSP00000268797          GFOD2     Q3B7J2           81577\n"
     ]
    }
   ],
   "source": [
    "all_mapped_string_ids_to_entrez = pd.DataFrame(columns=['STRING_ID', 'Protein_Symbol', 'Uniprot_ID', 'Entrez_Gene_ID'])\n",
    "stringids_no_entrez = []\n",
    "for stringid in known_STRING_ids:\n",
    "    protein_symbol = stringid_to_protsym[stringid]\n",
    "    entrez_id = ''\n",
    "    uniprot_id = '-'\n",
    "    if stringid in mapped_uniprot['From'].values:\n",
    "        # get the row index of where stringid is in the From column\n",
    "        row_index = mapped_uniprot[mapped_uniprot['From'] == stringid].index[0]\n",
    "        uniprot_id = mapped_uniprot['Entry'][row_index]\n",
    "    \n",
    "    if stringid in string_via_protsym_to_entrez:\n",
    "        entrez_id = string_via_protsym_to_entrez[stringid]\n",
    "        new_row_dict = {'STRING_ID': stringid, 'Protein_Symbol': protein_symbol, 'Uniprot_ID': uniprot_id, 'Entrez_Gene_ID': entrez_id}\n",
    "        all_mapped_string_ids_to_entrez.loc[len(all_mapped_string_ids_to_entrez)] = new_row_dict\n",
    "    elif uniprot_id in uniprot_ids_to_gene_ids:\n",
    "        for gene_id in uniprot_ids_to_gene_ids[uniprot_id]:\n",
    "            new_row_dict = {'STRING_ID': stringid, 'Protein_Symbol': protein_symbol, 'Uniprot_ID': uniprot_id, 'Entrez_Gene_ID': gene_id}\n",
    "            all_mapped_string_ids_to_entrez.loc[len(all_mapped_string_ids_to_entrez)] = new_row_dict\n",
    "    else:\n",
    "        stringids_no_entrez.append(stringid)\n",
    "    \n",
    "print('Number of known STRING IDs not mapped to entrez IDs: ' + str(len(stringids_no_entrez)))\n",
    "print('Number of mapped STRING IDs to entrez IDs (including multiple entrez IDs for the same STRING): ' + str(len(all_mapped_string_ids_to_entrez)))\n",
    "print('Number of unique STRING IDs that are mapped to entrez IDs: ' + str(len(all_mapped_string_ids_to_entrez['STRING_ID'].unique())))\n",
    "print(all_mapped_string_ids_to_entrez.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to file\n",
    "all_mapped_string_ids_to_entrez.to_csv('../data_processed/string_ids_prot_entrez.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to preprocess the NCI-ALMANAC dataset to include HSA and ZIP scores from DrugComb, code for this is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/w3wk1pc96f70nng8j2j_0c440000gn/T/ipykernel_44931/2972295059.py:1: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  almanac_df = pd.read_csv('../data/NCI-ALMANAC/ComboDrugGrowth_Nov2017.csv')\n",
      "/var/folders/6g/w3wk1pc96f70nng8j2j_0c440000gn/T/ipykernel_44931/2972295059.py:6: DtypeWarning: Columns (2,7,19,22,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  drugcomb_df = pd.read_csv('../data/DrugComb/drugcomb_summary_v_1_5.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DrugComb:  (311604, 26)\n",
      "With Conc, Filtered NCI-ALMANAC:  (2871444, 7)\n",
      "ComboScored, Filtered NCI-ALMANAC:  (311466, 5)\n"
     ]
    }
   ],
   "source": [
    "almanac_df = pd.read_csv('../data/NCI-ALMANAC/ComboDrugGrowth_Nov2017.csv')\n",
    "almanac_df = almanac_df.dropna(subset=['NSC1', 'NSC2', 'CELLNAME', 'PERCENTGROWTH', 'PANEL', 'SCORE'])\n",
    "almanac_df['NSC1'] = almanac_df['NSC1'].apply(lambda x: int(x))\n",
    "almanac_df['NSC2'] = almanac_df['NSC2'].apply(lambda x: int(x))\n",
    "\n",
    "drugcomb_df = pd.read_csv('../data/DrugComb/drugcomb_summary_v_1_5.csv')\n",
    "drugcomb_df = drugcomb_df[drugcomb_df['study_name'] == 'ALMANAC']\n",
    "drugcomb_df = drugcomb_df.dropna(subset=['drug_row', 'drug_col', 'cell_line_name', 'synergy_zip', 'synergy_hsa'])\n",
    "\n",
    "names_to_nsc = {}\n",
    "with open('../data/NCI-ALMANAC/ComboCompoundNames_all.txt') as f: # This file had some issues with the formatting, may have to replace the last four tabs manually\n",
    "    for line in f:\n",
    "        entries = line.split('\\t')\n",
    "        names_to_nsc[entries[1].strip('\\n').lower()] = int(entries[0])\n",
    "\n",
    "manual_drug_names_to_nsc = { # Found missing names to NSCs via PubChem/Google Searches\n",
    "    'sunitinib': 750690,\n",
    "    'cis-platin': 119875,\n",
    "    'nsc707389': 707389,\n",
    "    'navelbine ditartrate (tn)': 608210,\n",
    "    'eloxatin (tn) (sanofi synthelab)': 266046,\n",
    "    'nsc256439': 256439,\n",
    "    'fludarabine base': 118218,\n",
    "    'nsc733504': 733504,\n",
    "    'nsc-127716': 127716,\n",
    "    '5-aminolevulinic acid hydrochloride': 18509,\n",
    "    'chembl277800': 69,\n",
    "    'emcyt (pharmacia)': 702294,\n",
    "    'carboplatinum': 241240, # carboplatin\n",
    "    'chembl17639':105014,\n",
    "    '23541-50-6': 82151, # daunorubicin\n",
    "    '55-86-7': 762, # mechlorethamine\n",
    "    '7803-88-5': 66381, #6'-O-Methylguanosine\n",
    "    '158798-73-3': 712807, # capecitabine\n",
    "    '122111-05-1': 613327, # gemcitabine\n",
    "}\n",
    "names_to_nsc.update(manual_drug_names_to_nsc)\n",
    "\n",
    "# Map all drug_row and drug_col names to NSC numbers\n",
    "drugcomb_df['drug_row'] = drugcomb_df['drug_row'].apply(lambda x: names_to_nsc[x.lower()])\n",
    "drugcomb_df['drug_col'] = drugcomb_df['drug_col'].apply(lambda x: names_to_nsc[x.lower()])\n",
    "\n",
    "# What is the shape of the remaining DrugComb data?\n",
    "print(\"Filtered DrugComb: \", drugcomb_df.shape)\n",
    "\n",
    "almanac_cell_lines_to_drugcomb = { # Manually map some NCI-ALMANAC cell line names to DrugComb cell line names\n",
    "    'A549/ATCC': 'A549',\n",
    "    'NCI-H23': 'NCIH23',\n",
    "    'MDA-MB-231/ATCC': 'MDA-MB-231',\n",
    "    'UACC-62': 'UACC62',\n",
    "    'HCT-116': 'HCT116',\n",
    "    'OVCAR-3': 'OVCAR3',\n",
    "}\n",
    "\n",
    "almanac_df['CELLNAME'] = almanac_df['CELLNAME'].apply(lambda x: almanac_cell_lines_to_drugcomb[x] if x in almanac_cell_lines_to_drugcomb else x)\n",
    "almanac_df = almanac_df[almanac_df['CELLNAME'] != 'SF-539\\x1a'] # There is no DrugComb cell line data for 'SF-539\\x1a', so we drop it\n",
    "almanac_concentrations_df = almanac_df[['NSC1', 'CONC1', 'NSC2', 'CONC2', 'CELLNAME', 'PANEL', 'PERCENTGROWTH']]\n",
    "almanac_scores_df = almanac_df.groupby(['NSC1', 'NSC2', 'CELLNAME', 'PANEL'], as_index=False)['SCORE'].sum().rename(columns={'SCORE': 'COMBOSCORE'})\n",
    "\n",
    "# What is the shape of the remaining NCI-ALMANAC for the percent growth data?\n",
    "print(\"With Conc, Filtered NCI-ALMANAC: \", almanac_concentrations_df.shape)\n",
    "\n",
    "# What is the shape of the ComboScore NCI-ALMANAC data?\n",
    "print(\"ComboScored, Filtered NCI-ALMANAC: \", almanac_scores_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique NSC1, NSC2, CELLNAME pairs: 311466\n",
      "Number of entries with multiple matching scores: 55\n",
      "Number of entries with multiple flipped matching scores: 1981\n",
      "Number of unique entries with one ZIP score and one HSA score: 275821\n"
     ]
    }
   ],
   "source": [
    "# ZIP and HSA synergy scores are not always order invariant to which drug is used in drug_row vs drug_col\n",
    "# Get unique NSC1, NSC2, CELLNAME pairs from almanac_df\n",
    "unique_triplicates = almanac_scores_df[['NSC1', 'NSC2', 'CELLNAME']].drop_duplicates().reset_index(drop=True)\n",
    "print('Number of unique NSC1, NSC2, CELLNAME pairs:', len(unique_triplicates))\n",
    "\n",
    "# Create dictionaries for fast lookups\n",
    "first_way_dict = defaultdict(list)  # For (drug_row, drug_col, cell_line)\n",
    "second_way_dict = defaultdict(list) # For (drug_col, drug_row, cell_line)\n",
    "\n",
    "# Populate the dictionaries in a single pass through drugcomb_df\n",
    "for _, row in drugcomb_df.iterrows():\n",
    "    drug_row = row['drug_row']\n",
    "    drug_col = row['drug_col']\n",
    "    cell_line = row['cell_line_name']\n",
    "    zip_score = row['synergy_zip']\n",
    "    hsa_score = row['synergy_hsa']\n",
    "    \n",
    "    # Add to first way dictionary\n",
    "    first_way_dict[(drug_row, drug_col, cell_line)].append((zip_score, hsa_score))\n",
    "    \n",
    "    # Add to second way dictionary (for the flipped case)\n",
    "    second_way_dict[(drug_col, drug_row, cell_line)].append((zip_score, hsa_score))\n",
    "\n",
    "# Initialize empty columns\n",
    "unique_triplicates['First_way_ZIP'] = np.nan\n",
    "unique_triplicates['First_way_HSA'] = np.nan\n",
    "unique_triplicates['Second_way_ZIP'] = np.nan\n",
    "unique_triplicates['Second_way_HSA'] = np.nan\n",
    "\n",
    "# Track multiple scores\n",
    "multiple_matching_score_triplicates = set()\n",
    "multiple_flipped_matching_score_triplicates = set()\n",
    "\n",
    "# Process all rows efficiently using the dictionaries\n",
    "for idx, row in unique_triplicates.iterrows():\n",
    "    drug_row = row['NSC1']\n",
    "    drug_col = row['NSC2']\n",
    "    cell_line = row['CELLNAME']\n",
    "    \n",
    "    # Check first way\n",
    "    first_way_matches = first_way_dict.get((drug_row, drug_col, cell_line), [])\n",
    "    if len(first_way_matches) > 1:\n",
    "        multiple_matching_score_triplicates.add((drug_row, drug_col, cell_line))\n",
    "    elif len(first_way_matches) == 1:\n",
    "        zip_score, hsa_score = first_way_matches[0]\n",
    "        unique_triplicates.at[idx, 'First_way_ZIP'] = zip_score\n",
    "        unique_triplicates.at[idx, 'First_way_HSA'] = hsa_score\n",
    "    \n",
    "    # Check second way (flipped)\n",
    "    second_way_matches = second_way_dict.get((drug_row, drug_col, cell_line), [])\n",
    "    if len(second_way_matches) > 1:\n",
    "        multiple_flipped_matching_score_triplicates.add((drug_row, drug_col, cell_line))\n",
    "    elif len(second_way_matches) == 1:\n",
    "        zip_score, hsa_score = second_way_matches[0]\n",
    "        unique_triplicates.at[idx, 'Second_way_ZIP'] = zip_score\n",
    "        unique_triplicates.at[idx, 'Second_way_HSA'] = hsa_score\n",
    "\n",
    "# Print statistics\n",
    "print('Number of entries with multiple matching scores:', len(multiple_matching_score_triplicates))\n",
    "print('Number of entries with multiple flipped matching scores:', len(multiple_flipped_matching_score_triplicates))\n",
    "\n",
    "# Retrieve the unique entries that only have one ZIP score and one HSA score\n",
    "unique_singleziphsa_entries = unique_triplicates[\n",
    "    (unique_triplicates['First_way_ZIP'].notna() ^ unique_triplicates['Second_way_ZIP'].notna()) &\n",
    "    (unique_triplicates['First_way_HSA'].notna() ^ unique_triplicates['Second_way_HSA'].notna())\n",
    "]\n",
    "\n",
    "unique_singleziphsa_entries = unique_triplicates[\n",
    "    (unique_triplicates['First_way_ZIP'].notna() ^ unique_triplicates['Second_way_ZIP'].notna()) &\n",
    "    (unique_triplicates['First_way_HSA'].notna() ^ unique_triplicates['Second_way_HSA'].notna())\n",
    "].copy()\n",
    "\n",
    "unique_singleziphsa_entries.loc[:, 'ZIP'] = unique_singleziphsa_entries.apply(\n",
    "    lambda row: row['First_way_ZIP'] if pd.notna(row['First_way_ZIP']) else row['Second_way_ZIP'], axis=1\n",
    ")\n",
    "\n",
    "unique_singleziphsa_entries.loc[:, 'HSA'] = unique_singleziphsa_entries.apply(\n",
    "    lambda row: row['First_way_HSA'] if pd.notna(row['First_way_HSA']) else row['Second_way_HSA'], axis=1\n",
    ")\n",
    "unique_singleziphsa_entries = unique_singleziphsa_entries.drop(columns=['First_way_ZIP', 'First_way_HSA', 'Second_way_ZIP', 'Second_way_HSA'])\n",
    "print('Number of unique entries with one ZIP score and one HSA score:', len(unique_singleziphsa_entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the almanac_scores_df with the unique_singleziphsa_entries after the rightmost column\n",
    "merged_scores_df = pd.merge(\n",
    "    unique_singleziphsa_entries,\n",
    "    almanac_scores_df,\n",
    "    how='right',\n",
    "    left_on=['NSC1', 'NSC2', 'CELLNAME'],\n",
    "    right_on=['NSC1', 'NSC2', 'CELLNAME'],\n",
    "    suffixes=('', '_y')\n",
    ")\n",
    "# Drop the extra columns from the right DataFrame\n",
    "merged_scores_df = merged_scores_df.loc[:, ~merged_scores_df.columns.str.endswith('_y')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the concentration dataframe to a CSV file\n",
    "almanac_concentrations_df.to_csv('../data_processed/almcomb_concentrations.csv', index=False)\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "merged_scores_df.to_csv('../data_processed/almcomb_comboscore_hsa_zip.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
